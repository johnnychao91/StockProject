import pandas as pd
import feedparser
import os
from datetime import datetime, timedelta

# æœç´¢é—œéµå­—
query = "nvidia"
# query = "tsmc"
# query = "nasdaq"
# query = "trump"

# è¨­å®šæŸ¥è©¢æ™‚é–“ç¯„åœ
start_date = datetime(2023, 1, 1)  # é–‹å§‹æ—¥æœŸ
end_date = datetime.today()  # çµæŸæ—¥æœŸ

# è¨­å®šå­˜å„²ç›®éŒ„
base_dir = f"./data/google_news/{query}/"
os.makedirs(base_dir, exist_ok=True)  # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨

# é€æ—¥ç²å–æ–°è
current_date = start_date
while current_date <= end_date:
    # è¨­å®š CSV æª”æ¡ˆåç¨±
    file_name = current_date.strftime("%Y_%m_%d") + ".csv"
    file_path = os.path.join(base_dir, file_name)

    # **æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨æª”æ¡ˆï¼Œè‹¥å­˜åœ¨å‰‡è·³é**
    if os.path.exists(file_path):
        print(f"{file_path} å·²å­˜åœ¨ï¼Œè·³é...")
        current_date += timedelta(days=1)
        continue  # è·³éç•¶å¤©ï¼Œç›´æ¥é€²å…¥ä¸‹ä¸€å¤©

    # è¨­å®š `after` å’Œ `before`ï¼ˆåªæŸ¥è©¢ç•¶æ—¥ï¼‰
    after_str = current_date.strftime("%Y-%m-%d")
    before_str = (current_date + timedelta(days=1)).strftime("%Y-%m-%d")

    # Google News RSS URLï¼ˆæŸ¥è©¢ç•¶æ—¥æ–°èï¼‰
    rss_url = f"https://news.google.com/rss/search?q={query}+after:{after_str}+before:{before_str}&hl=en-US&gl=US&ceid=US:en"

    # è§£æ RSS
    news_feed = feedparser.parse(rss_url)

    # è§£ææ–°èè³‡æ–™
    news_data = []
    articles = news_feed.entries[:100]  # é™åˆ¶æ¯å¤©æœ€å¤š 100 ç¯‡
    for entry in articles:
        try:
            # è§£æç™¼ä½ˆæ™‚é–“
            published_time = entry.published if "published" in entry else "æœªçŸ¥"

            # è½‰æ›æˆ datetime æ ¼å¼
            if published_time != "æœªçŸ¥":
                pub_date = datetime.strptime(published_time, "%a, %d %b %Y %H:%M:%S %Z")
            else:
                pub_date = None

            # **å°å‡ºå®Œæ•´çš„æ–°èè³‡è¨Š**
            print("\nğŸ” ç™¼ç¾æ–°è:")
            print(f"æ¨™é¡Œ: {entry.get('title', 'N/A')}")
            print(f"ç™¼ä½ˆæ™‚é–“: {pub_date.strftime('%Y-%m-%d %H:%M:%S') if pub_date else 'æœªçŸ¥'}")
            print(f"æè¿°: {entry.get('summary', 'N/A')}")
            print(f"ä¾†æº: {entry.get('source', {}).get('title', 'æœªçŸ¥') if 'source' in entry else 'æœªçŸ¥'}")
            print(f"é€£çµ: {entry.get('link', 'N/A')}")
            print(f"GUID: {entry.get('id', 'N/A')}")
            print(f"é¡åˆ¥: {entry.get('category', 'N/A')}")
            print(f"æ¨™ç±¤: {[tag['term'] for tag in entry.get('tags', [])] if 'tags' in entry else 'N/A'}")
            print(f"åœ–ç‰‡: {entry.get('media_content', 'N/A')}")
            print(f"æ›´æ–°æ™‚é–“: {entry.get('updated', 'N/A')}")
            print("-" * 80)

            # æª¢æŸ¥æ–°èæ™‚é–“æ˜¯å¦åœ¨ç¯„åœå…§
            if pub_date and start_date <= pub_date <= end_date:
                news_data.append({
                    "ç™¼ä½ˆæ™‚é–“": pub_date.strftime("%Y-%m-%d %H:%M:%S"),
                    "æ¨™é¡Œ": entry.get("title", "N/A"),
                    "æè¿°": entry.get("summary", "N/A"),
                    "ä¾†æº": entry.get("source", {}).get("title", "æœªçŸ¥") if "source" in entry else "æœªçŸ¥",
                    "é€£çµ": entry.get("link", "N/A"),
                    "GUID": entry.get("id", "N/A"),
                    "é¡åˆ¥": entry.get("category", "N/A"),
                    "æ¨™ç±¤": [tag["term"] for tag in entry.get("tags", [])] if "tags" in entry else "N/A",
                    "åœ–ç‰‡": entry.get("media_content", "N/A"),
                    "æ›´æ–°æ™‚é–“": entry.get("updated", "N/A"),
                })

        except Exception as e:
            print(f"ç²å– {after_str} çš„æ–°èæ™‚å‡ºéŒ¯: {e}")

    # å¦‚æœç•¶å¤©æœ‰æ–°èï¼Œå‰‡å­˜å…¥ CSV
    if news_data:
        # è½‰æ›ç‚º DataFrame
        df = pd.DataFrame(news_data)

        # å„²å­˜ CSVï¼ˆé¿å…ç·¨ç¢¼å•é¡Œï¼Œä½¿ç”¨ utf-8-sigï¼‰
        df.to_csv(file_path, index=False, encoding="utf-8-sig")
        print(f"âœ… æ–°èå·²å„²å­˜ç‚º {file_path}")

    else:
        print(f"âš ï¸ {after_str} æ²’æœ‰æ–°èå¯å„²å­˜")

    # é€²å…¥ä¸‹ä¸€å¤©
    current_date += timedelta(days=1)
